{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on OQMD \n",
    "\n",
    "Abhijith Gopakumar, *Northwestern University*\n",
    "\n",
    "\n",
    "## Part 1: Querying OQMD and Retrieving data using OPTIMADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies: these can also be found in the `requirements.txt` file in this folder\n",
    "%pip install numpy scikit-learn requests matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import requests\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL for OPTIMADE REST queries\n",
    "\n",
    "rest_base = \"http://oqmd.org/optimade/structures?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query URL with filter, response_fields and paging requirements\n",
    "\n",
    "# The following query filters for data of ternary non-metallic oxides\n",
    "# Crystal structure parameters and Band gap values are returned in response_fields\n",
    "# Crystal structures will be used to generate representational vectors (input features) for ML\n",
    "# Bandgap values will be used as targets for ML\n",
    "\n",
    "\n",
    "filter_   = '_oqmd_stability<=0 AND elements HAS \"O\" AND nelements=3 AND _oqmd_band_gap>0'\n",
    "\n",
    "response_ = 'id,_oqmd_entry_id,lattice_vectors,cartesian_site_positions,species_at_sites,_oqmd_band_gap'\n",
    "\n",
    "page_     = [\"page_offset=0\", \"page_limit=200\"]\n",
    "\n",
    "filter_   = 'filter=' + filter_\n",
    "response_ = 'response_fields=' + response_\n",
    "\n",
    "oqmd_optimade_query = rest_base + \"&\".join([filter_, response_]+page_)\n",
    "print(\"Created Query: \\n\\n{}\".format(oqmd_optimade_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do a test query on URL using requests.get() This can take a few minutes.\n",
    "\n",
    "response = requests.get(oqmd_optimade_query)\n",
    "if response.status_code == 200:\n",
    "    print(\"Success!\")\n",
    "    #print(response.json())\n",
    "else:\n",
    "    print(\"Query failed. Status: {}\".format(response.status_code))\n",
    "    print(\"Error Message: {}\".format(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need more than 200 datapoints for machine learning - if more data is available\n",
    "\n",
    "# As the first step, here's the same script from the cell above, but kept inside a function \n",
    "\n",
    "def query_oqmd_optimade(query):\n",
    "    print(\"\\nQuerying: {}\".format(query))\n",
    "    response = requests.get(query)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Success!\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Query failed. Status: {}\".format(response.status_code))\n",
    "        print(\"Error Message: {}\".format(response.text))\n",
    "        return \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Next, we query for 1000 materials in total using 5 sequential API queries - each paginated to\n",
    "# retrieve 200 materials\n",
    "\n",
    "\n",
    "load_data_from_saved = True  \n",
    "# This is to avoid querying OQMD repeatedly for the same data, if .\n",
    "# Because the data I downloaded is already available as a JSON file in this Git repo.\n",
    "\n",
    "# But if you'd like to try out querying OQMD, set \"load_data_from_saved\" as \"False\"\n",
    "# Querying OQMD for this particular data would take about 5-10 minutes to complete\n",
    "\n",
    "\n",
    "dataset_filename = Path(os.path.realpath(\".\")).joinpath(\"./oqmd_exercise_data/raw_dataset_saved.json\")\n",
    "\n",
    "# Check if the file exists:\n",
    "\n",
    "if load_data_from_saved:\n",
    "    if os.path.isfile(dataset_filename):\n",
    "        with open(dataset_filename, 'r') as fin:\n",
    "            dataset = json.load(fin)\n",
    "    else:        # If the file does not exist try to load the file from github\n",
    "        response = requests.get('https://raw.githubusercontent.com/Materials-Consortia/optimade-tutorial-exercises/main/notebooks/oqmd_exercise_data/raw_dataset_saved.json')\n",
    "        if response.status_code == 200:\n",
    "            dataset = json.loads(deepcopy(response.content))\n",
    "        else:\n",
    "            load_data_from_saved = False\n",
    "\n",
    "if not load_data_from_saved:\n",
    "    dataset = []\n",
    "    query = oqmd_optimade_query\n",
    "    for i in range(5):\n",
    "        jsondata = query_oqmd_optimade(query)\n",
    "        if jsondata is None:\n",
    "            break\n",
    "        else:\n",
    "            # Get the link to the next page and query it in next loop iteration\n",
    "            query = deepcopy(jsondata['links']['next'])\n",
    "            dataset.append(deepcopy(jsondata))\n",
    "    with open(dataset_filename, 'w') as fout:\n",
    "        json.dump(dataset, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the paginated response - checking reliability of server-side and client side scripts\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    query = dataset[i]['meta']['query']['representation']\n",
    "    page_params =  [param for param in query.split(\"&\") if param.startswith(\"page\")]\n",
    "    print(\"{}:{}\".format(i,\", \".join(page_params)))\n",
    "    #print(dataset[i]['meta']['query']['_oqmd_final_query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the response data keys and confirm all the necessary information is available\n",
    "\n",
    "print(dataset[0]['data'][0].keys())\n",
    "print(dataset[0]['data'][0]['attributes'].keys())\n",
    "print(dataset[0]['meta'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the type of the returned data is OPTIMADE's \"structures\"\n",
    "print(dataset[0]['data'][1]['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to convert OPTIMADE's structure data to POSCAR. \n",
    "\n",
    "# Make sure that 'lattice_vectors', 'species_at_sites', and 'cartesian_site_positions' are\n",
    "# included in the response_fields of query URL\n",
    "\n",
    "def get_poscar_from_optimade_structure(structure):\n",
    "    if '_oqmd_entry_id' in structure['attributes'].keys():\n",
    "        poscar =  [\"REST API StructureID {}, OQMD Entry ID {}\".format(\n",
    "            structure['id'], structure['attributes']['_oqmd_entry_id']\n",
    "        )]\n",
    "        filename = \"ID-{}_OQMD-EnID-{}.poscar\".format(structure['id'],structure['attributes']['_oqmd_entry_id'])\n",
    "    else:\n",
    "        poscar =  [\"REST API StructureID {}\".format(structure['id'])]\n",
    "        filename = \"ID-{}.poscar\".format(structure['id'])\n",
    "        \n",
    "    poscar.append(\"1.0\")\n",
    "    \n",
    "    poscar += [\" \".join([str(jtem) for jtem in item]) \n",
    "               for item in structure['attributes']['lattice_vectors']\n",
    "              ]\n",
    "    \n",
    "    elems  = []\n",
    "    counts = []\n",
    "    for item in structure['attributes']['species_at_sites']:\n",
    "        if item in elems:\n",
    "            assert elems.index(item) == len(elems)-1\n",
    "            counts[-1] += 1\n",
    "        else:\n",
    "            elems.append(deepcopy(item))\n",
    "            counts.append(1)\n",
    "    poscar.append(\" \".join(elems))\n",
    "    poscar.append(\" \".join([str(item) for item in counts]))\n",
    "    \n",
    "    poscar.append(\"Cartesian\")\n",
    "    \n",
    "    poscar += [\" \".join([str(jtem) for jtem in item]) \n",
    "               for item in structure['attributes']['cartesian_site_positions']\n",
    "              ]\n",
    "    poscar = \"\\n\".join(poscar)\n",
    "    return (poscar, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OPTIMADE structure -> POSCAR conversion function \n",
    "# and save all structures in directory \"./input_poscars\"\n",
    "\n",
    "# Also save the bandgap values in a file \"target_properties.csv\"\n",
    "\n",
    "poscar_dir = \"./input_poscars\"\n",
    "if os.path.exists(poscar_dir):\n",
    "    shutil.rmtree(poscar_dir)\n",
    "os.mkdir(poscar_dir)\n",
    "\n",
    "properties = []\n",
    "\n",
    "for dt in dataset:\n",
    "    for st in dt['data']:\n",
    "        poscar, filename = get_poscar_from_optimade_structure(deepcopy(st))\n",
    "        target_value = deepcopy(st['attributes']['_oqmd_band_gap'])\n",
    "        properties.append(\",\".join([filename,str(target_value)]))\n",
    "        with open(os.path.join(poscar_dir,filename),\"w\") as fout:\n",
    "            fout.write(poscar)\n",
    "with open(\"target_properties.csv\",\"w\") as fout:\n",
    "    fout.write(\"filename, _oqmd_band_gap \\n\")\n",
    "    fout.write(\"\\n\".join(properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to generate a set of material representation vectors (input features) from the POSCAR data for Machine Learning\n",
    "\n",
    "#### The set of features generated for this tutorial is given in the file  \"features_processed_final.csv\" in the Github repo\n",
    "\n",
    "\n",
    "### Optional\n",
    "#### An example set of steps to generate Magpie material features is shown below. The following commands are to be executed on a bash shell from the same directory where this jupyter notebook resides. \n",
    "\n",
    "```\n",
    "git clone git@github.com:tachyontraveler/magpie_workflow.git\n",
    "\n",
    "> Cloning into 'magpie_workflow'...\n",
    ">\n",
    "> remote: Enumerating objects: 115, done.\n",
    ">\n",
    "> remote: Counting objects: 100% (6/6), done.\n",
    ">\n",
    "> remote: Compressing objects: 100% (6/6), done.\n",
    ">\n",
    "> remote: Total 115 (delta 2), reused 0 (delta 0), pack-reused 109\n",
    ">\n",
    "> Receiving objects: 100% (115/115), 16.03 MiB | 3.25 MiB/s, done.\n",
    ">\n",
    "> Resolving deltas: 100% (31/31), done.\n",
    "\n",
    "$ cp -r ./input_poscars magpie_workflow/\n",
    "\n",
    "$ cd magpie_workflow\n",
    "\n",
    "$ python3 workflow.py \n",
    "\n",
    ">Initializing the workflow class\n",
    ">\n",
    "> 2021-11-22 19:51:45.614116 ::  Generating property.txt file\n",
    ">\n",
    "> 2021-11-22 19:51:45.616918 ::  Generating Magpie input commands file \n",
    ">\n",
    "> 2021-11-22 19:51:45.617025 ::  Magpie input file created as ./OUTDIR/generate-attributes.in\n",
    ">\n",
    "> 2021-11-22 19:51:45.617060 ::  Calling Magpie with the input script.. \n",
    ">\n",
    ">2021-11-22 19:51:45.617102 ::  May check out.workflow.txt for Magpie messages\n",
    ">\n",
    ">2021-11-22 19:52:36.347243 ::  Finished Magpie generation. Now post-processing\n",
    ">\n",
    ">2021-11-22 19:52:36.399678 ::  Done\n",
    "\n",
    "$ cd ../\n",
    "\n",
    "$ cp magpie_workflow/OUTDIR/features_processed_final.csv ./\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Machine Learning\n",
    "\n",
    "#### Now we can proceed to build a small ML model using Scikit Learn library modules and fit it on the data obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data generated in previous sections\n",
    "\n",
    "# Input feature data - generated using Magpie on POSCAR files\n",
    "if not os.path.isfile(\"features_processed_final.csv\"):\n",
    "    response = requests.get('https://raw.githubusercontent.com/Materials-Consortia/optimade-tutorial-exercises/main/notebooks/oqmd_exercise_data/features_processed_final.csv')\n",
    "    if response.status_code == 200:\n",
    "        with open(\"features_processed_final.csv\",\"wb\") as fout:\n",
    "            fout.write((response.content))\n",
    "    else:\n",
    "        raise RuntimeError(\"Please complete the optional step above, or copy the example data from `oqmd_exercise_data` into this directory.\")\n",
    "\n",
    "features = open(\"./features_processed_final.csv\",\"r\").read().strip().split(\"\\n\")\n",
    "feats_title = features[0].strip().split(\",\")\n",
    "\n",
    "features = np.array([item.strip().split(\",\") for item in features[1:]])\n",
    "features = features[~np.isnan(features[:,1:].astype(float)).any(axis=1)]\n",
    "I = features[:,0]\n",
    "X = features[:,1:].astype(float)\n",
    "\n",
    "\n",
    "# Target data (bandgap values)\n",
    "targets = open(\"./target_properties.csv\").read().strip().split(\"\\n\")\n",
    "targets_title = targets[0].strip().split(\",\")\n",
    "\n",
    "targets = np.array([item.strip().split(\",\") for item in targets[1:]])\n",
    "targets = dict(zip(targets[:,0],targets[:,1].astype(float)))\n",
    "\n",
    "Y = [targets[item] for item in I]\n",
    "\n",
    "# The feats_title and targets_title are not used anywhere else in this tutorial. \n",
    "# But I'd recommend keeping them for tracking features and targets in an actual ML study\n",
    "# For example, they can be be useful in feature importance analysis or in multi-target modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and test sets\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, \n",
    "                                                test_size=0.2,\n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Scikit Learn pipeline with feature scaling, dimension reduction, and fianlly, a regressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "# Parameters to search in finding the best model\n",
    "# The following sets of parameters are just for a shallow search for optimization\n",
    "# A finer search for parameters will be required on an actual ML study\n",
    "\n",
    "params = {\n",
    "    'pca__n_components': [200,250,xtrain.shape[-1]],\n",
    "    'svr__C':[0.1,1,10,20],\n",
    "    'svr__kernel':['rbf'],\n",
    "    'svr__epsilon':[0.1],\n",
    "    'svr__gamma':['scale']\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(pipeline, params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the grid search and get scores\n",
    "\n",
    "gridsearch.fit(xtrain, ytrain)\n",
    "\n",
    "print('Traindata score: {}'.format(gridsearch.score(xtrain, ytrain)))\n",
    "print('Testdata score: {}'.format(gridsearch.score(xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (I'm aware that the test data score is much lower that of traindata - implying a possible overfit.  But this would work just fine as a representational ML modeling workflow for this tutorial session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which set of paramters had the best fit as of now\n",
    "print(gridsearch.best_estimator_)\n",
    "\n",
    "# Further, a finer search for most optimum set of paramters is required to get a better model.\n",
    "# But for the sake of this tutorial, I'm gonna continue to plot the testdata predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test data\n",
    "\n",
    "ytest_pred = gridsearch.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "plt.scatter(ytest, ytest_pred, \n",
    "            color='teal', \n",
    "            alpha=0.7)\n",
    "plt.xlabel(\"DFT Band gap (eV)\")\n",
    "plt.ylabel(\"ML-Predicted Band gap (eV)\")\n",
    "xmax = max([max(ytest),max(ytest_pred)])\n",
    "plt.plot([0,xmax],[0,xmax],color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the plots\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it for now, folks!\n",
    "I hope this notebook helped to get started on retrieving OQMD data via OPTIMADE API and using it to build a quick ML model.\n",
    "\n",
    "I will try to add more descriptive information and enhancements to this tutorial in the future.\n",
    "\n",
    "Let me know if you have any questions!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optimade_oqmd_exercise]",
   "language": "python",
   "name": "conda-env-optimade_oqmd_exercise-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
